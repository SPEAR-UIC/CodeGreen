----- LASSI RESPONSE: code comparison -----

Below are some key differences introduced in the refactored code that contribute toward improved energy efficiency:

• Use of Pinned Host Memory:
 – The refactored code allocates host buffers using cudaHostAlloc instead of malloc.
 – This allows faster and asynchronous data transfers between host and device, reducing CPU idle time and potentially lowering energy consumption.

• Asynchronous Data Transfers & Stream Usage:
 – The refactored version creates a CUDA stream and uses cudaMemcpyAsync for copying data in and out of the device.
 – Asynchronous transfers can overlap communication with computation, which minimizes waiting periods and improves energy efficiency.

• Constant Memory for Query Bounds:
 – Instead of copying the lower and upper arrays into global memory, they are stored in constant memory (d_lower_const and d_upper_const).
 – Constant memory typically offers lower latency and decreased memory power consumption when many threads access the same data.

• Improved Arithmetic Optimization:
 – The refactored kernel computes the reciprocal (rec_std = 1.0f / std) once per window, reducing the number of divisions in the inner loop.
 – Fewer expensive operations per iteration can lower both execution time and energy use.

• Kernel Simplification and Reduced Redundancy:
 – The refactored kernel removes an in-kernel loop that repeated computations (the repeat parameter is kept for API compatibility but not used for repeated internal execution).
 – Doing less redundant work per kernel launch contributes to shorter kernel run times and hence lower energy consumption.

These changes—together with cooperative loading of tiles into shared memory and ensuring proper synchronization with asynchronous copies—enable the refactored code to complete in less total runtime and with lower overall energy consumption, even if the kernel “average execution time” appears slightly different.
