--- LASSI RESPONSE: code comparison -----

Below are the key changes in the refactored code that are likely to contribute to improved energy efficiency:

Use of Constant Memory for the Log Table:
 • In the refactored version, the logarithm lookup table is copied into device constant memory (d_logTableConst) rather than regular global memory.
• Constant memory is cached and optimized for broadcast access by all threads, which minimizes memory traffic and reduces energy spent on memory accesses.

Early Thread Exit and Bound Checking:
 • The baseline kernel in the refactored code begins with an early exit if a thread’s global coordinates fall outside the valid image bounds.
• This ensures that threads not needed for computation do not perform any extra work, reducing unnecessary GPU activity and saving power.
Loop Unrolling: • The inner loops over the 5×5 window are annotated with "#pragma unroll" so the compiler can unroll them.
• Unrolling reduces the loop overhead and can increase instruction throughput, leading to faster execution and lower energy use per computation.

Pinned Host Memory and Asynchronous Transfers:
 • The refactored version uses pinned (page-locked) memory for host allocations (via cudaHostAlloc), which improves data transfer performance between host and device.
• Additionally, asynchronous memory copies (cudaMemcpyAsync) are used together with a dedicated CUDA stream.
• These modifications help increase throughput and lower idle waiting time, benefiting overall system power consumption.

Use of CUDA Streams and Events for Efficient Synchronization:
 • A dedicated CUDA stream is created to better overlap computation and data transfers, reducing idle GPU time.
• Timing is handled with CUDA events (cudaEventRecord and cudaEventElapsedTime) rather than the CPU’s chrono timers.
• These choices can lead to reduced kernel launch latencies and overall execution time, resulting in decreased energy usage.

Optimized Shared Memory Usage with a Tiled Approach:
 • In the optimized kernel, the refactored code tiles the input into shared memory with a halo region.
• By loading a shared tile (including halo) in one go, redundant global memory accesses are reduced, which decreases both energy consumption and execution time.

Removal of Redundant Repeat Computations:
 • In the refactored baseline kernel, the entropy is computed only once rather than running the computation repeatedly (as signaled by the d_repeat constant), which cuts down on unnecessary repeated work.
 
Overall, these changes reduce memory latency, lower overhead in thread execution, and reduce the volume of redundant or unnecessary computations, all contributing to the substantial improvements in both execution time and energy/power consumption demonstrated by the test results.
